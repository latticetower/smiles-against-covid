{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This is a basic notebook with solution which uses SentenceTransformers library.\n",
    "For now I suppose that it is already installed. Later I'll make it Google Colab-runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here should be install commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import InputExample\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator, CECorrelationEvaluator\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryAccuracyEvaluator\n",
    "from sentence_transformers import util as st_util\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    pass\n",
    "\n",
    "# Cell with constants\n",
    "DATADIR = Path(\"../data\")\n",
    "if not DATADIR.exists():\n",
    "  # DATADIR.mkdir(DATADIR)\n",
    "  !gdown --id 1qnvNxd6SvhwHPxD0huTpmODB270ENs7j\n",
    "  !tar -xzvf inhibitors_data.tar.gz\n",
    "\n",
    "RANDOM_SEED = 2407\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "TMP_DIR = Path(\"../tmp\")\n",
    "TMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(DATADIR / \"train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(DATADIR / \"test.csv\", index_col=0)\n",
    "\n",
    "# train_df['canonical'] = train_df.Smiles.apply(smiles2canonical)\n",
    "# test_df['canonical'] = test_df.Smiles.apply(smiles2canonical)\n",
    "\n",
    "MODELNAME = TMP_DIR / \"embeddings\"\n",
    "CROSS_ENCODER_PATH = TMP_DIR / \"cross-encoder\"\n",
    "# the name of the baseline BERT model which is getting fine-tuned\n",
    "SMILES_COL = \"Smiles\"  # \"canonical\" \n",
    "NFOLDS = 5\n",
    "\n",
    "TRAIN_SIZE = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: (768,)\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: (768,)\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: (768,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = SentenceTransformer(MODEL_NAME)\n",
    "model = SentenceTransformer(MODELNAME.as_posix())\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding.shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on train dataset processing\n",
    "1. At first we will be using simple scheme which guarantiees that our train data is balanced. First strategy: each time we pick pairs of active and inactive molecules A, B and I, J. Add to the dataset (A, B, 1), (I, J, 1), and 2 random of [(A, I, 0), (A, J, 0), (B, I, 0), (B, J, 0)]. Or maybe triplet loss? Will check at the documentation later what is the dataset in that case.\n",
    "2. We can also take into account Murcko scaffold or some other form of scaffolds. And pick (some of) the molecules in the way to compare molecules with the same scaffold from different parts of dataset.\n",
    "3. For validation we might want to specifically use scaffolds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shuffled_dataset_for_fold(train_df, index, size=100, smiles_col=\"Smiles\"):\n",
    "    df = train_df.loc[index]\n",
    "    m = df.Active.mean()\n",
    "    assert m > 0 and m < 1\n",
    "    active_molecules = df.loc[df.Active, smiles_col].values\n",
    "    inactive_molecules = df.loc[~df.Active, smiles_col].values\n",
    "    new_data = []\n",
    "    for k in range(size//4):\n",
    "        A, B = np.random.choice(active_molecules, 2)\n",
    "        I, J = np.random.choice(inactive_molecules, 2)\n",
    "        new_data.extend([(A, B, 1), (I, J, 1)])\n",
    "        neg = [(A, I, 0), (A, J, 0), (B, I, 0), (B, J, 0)]\n",
    "        i, j = np.random.choice(len(neg), 2)\n",
    "        new_data.append(neg[i])\n",
    "        new_data.append(neg[j])\n",
    "    np.random.shuffle(new_data)\n",
    "    return new_data\n",
    "\n",
    "# for testing we are specifically interested in comparing molecules from the same scaffold\n",
    "# since the errors will probably be higher in this case\n",
    "\n",
    "def make_val_dataset_for_fold(val_df, val_index, subsample=True, smiles_col=\"Smiles\",\n",
    "        scaffold_col=None, max_size=None):\n",
    "    df = val_df.loc[val_index]\n",
    "    active_molecules = df.loc[df.Active, smiles_col].values\n",
    "    inactive_molecules = df.loc[~df.Active, smiles_col].values\n",
    "\n",
    "    val_data = []\n",
    "\n",
    "    if scaffold_col is None:\n",
    "        # don't use scaffolds, just compare all\n",
    "        for k, molecule_a in enumerate(active_molecules):\n",
    "            for molecule_i in inactive_molecules:\n",
    "                val_data.append((molecule_a, molecule_i, 0))\n",
    "                if max_size is not None and max_size <= len(val_data):\n",
    "                    return val_data\n",
    "            for molecule_b in inactive_molecules[k + 1:]:\n",
    "                if molecule_a == molecule_b:\n",
    "                    continue\n",
    "                val_data.append((molecule_a, molecule_b, 1))\n",
    "                if max_size is not None and max_size <= len(val_data):\n",
    "                    return val_data\n",
    "        for k, molecule_i in enumerate(inactive_molecules):\n",
    "            for molecule_j in inactive_molecules[k + 1:]:\n",
    "                val_data.append((molecule_i, molecule_j, 1))\n",
    "                if max_size is not None and max_size <= len(val_data):\n",
    "                    return val_data\n",
    "    else:\n",
    "        # do subsampling based on scaffolds\n",
    "        # idea: molecules from the same scaffold should be close to each other \n",
    "        # if they are all active or inactive\n",
    "        # and remote if they are diverse in terms of activity\n",
    "        scaffold_active = defaultdict(set)\n",
    "        for s, m in df.loc[df.Active, [scaffold_col, smiles_col]].values:\n",
    "            scaffold_active[s].add(m)\n",
    "        \n",
    "        scaffold_inactive = defaultdict(set)    \n",
    "        for s, m in df.loc[~df.Active, [scaffold_col, smiles_col]].values:\n",
    "            scaffold_inactive[s].add(m)\n",
    "        \n",
    "        all_scaffolds = df.loc[:, scaffold_col].unique()\n",
    "\n",
    "        for scaffold in all_scaffolds:\n",
    "            active_molecules = scaffold_active[scaffold]\n",
    "            inactive_molecules = scaffold_inactive[scaffold]\n",
    "            for ds in [active_molecules, inactive_molecules]:\n",
    "                for k, mol1 in enumerate(ds):\n",
    "                    for mol2 in ds[k + 1:]:\n",
    "                        val_data.append((mol1, mol2, 1))\n",
    "                        if max_size is not None and max_size <= len(val_data):\n",
    "                            return val_data\n",
    "            for mol_a in active_molecules:\n",
    "                for mol_i in inactive_molecules:\n",
    "                    val_data.append((mol_a, mol_i, 0))\n",
    "                    if max_size is not None and max_size <= len(val_data):\n",
    "                        return val_data\n",
    "    return val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "active_ids = np.where(train_df.Active == 1)[0]\n",
    "inactive_ids = np.where(train_df.Active == 0)[0]\n",
    "all_folds = [\n",
    "    (\n",
    "        [*active_ids[:3], *inactive_ids[:3]],\n",
    "        [*active_ids[-3:], *inactive_ids[-3:]]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "100\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 7/7 [00:44<00:00,  6.39s/it]\n",
      "Epoch: 100%|██████████| 1/1 [00:46<00:00, 46.94s/it]\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, val_index) in enumerate(all_folds):\n",
    "    #for fold, (train_index, val_index) in enumerate(all_folds):\n",
    "    print(\"Fold\", fold)\n",
    "    train_samples = make_shuffled_dataset_for_fold(train_df, train_index, size=TRAIN_SIZE)\n",
    "    print(len(train_samples))\n",
    "    val_samples = make_val_dataset_for_fold(\n",
    "        train_df, val_index, subsample=True,\n",
    "        max_size=10\n",
    "    )\n",
    "    print(len(val_samples))\n",
    "    # todo: uncommend to use murcko scaffold and add code to compute them\n",
    "    # test_samples = make_val_dataset_for_fold(train_df, val_index, subsample=True, scaffold_col=\"murcko\")\n",
    "    # print(train_index)\n",
    "    train_dataset = [\n",
    "        InputExample(texts=[i, j], label=w)  ##int(w > 0) ) \n",
    "        for (i, j, w) in train_samples\n",
    "    ]\n",
    "    # test_dataset = [\n",
    "    #     InputExample(texts=[i, j], label=w) # int(w > 0))\n",
    "    #     for k in val_index\n",
    "    #     for (i, j, w) in val_samples\n",
    "    #     if np.abs(w) >= 0.1\n",
    "    # ]\n",
    "    #val_dataset = [(i, j, np.clip((w+1.)/2, 0., 1)) for i in val_index for (i, j, w) in all_data[i]]\n",
    "    val_dataset = [\n",
    "        ([i, j], w)  #int(w > 0) ) \n",
    "        for (i, j, w) in val_samples\n",
    "    ]  # we don't use holdout dataset because we have too few data\n",
    "\n",
    "    #(sentences1, sentences2, scores) = list(zip(*val_dataset))\n",
    "    (sentences, scores) = list(zip(*val_dataset))\n",
    "    #evaluator = EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
    "    evaluator = CECorrelationEvaluator(sentences, scores)\n",
    "    #binary_evaluator = CEBinaryClassificationEvaluator(sentences[:100], scores[:100])\n",
    "    #evaluator = CEBinaryAccuracyEvaluator(sentences[:100], scores[:100])\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "    # val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=16)\n",
    "\n",
    "    #model = SentenceTransformer(MODELNAME)\n",
    "    #EleutherAI/gpt-neo-125M\n",
    "    #try:\n",
    "\n",
    "    model = CrossEncoder(CROSS_ENCODER_PATH, num_labels=1)\n",
    "    model.fit(\n",
    "        train_dataloader,\n",
    "        evaluator=evaluator,  #evaluator,\n",
    "        epochs=1,\n",
    "        loss_fct=nn.MSELoss(),\n",
    "        evaluation_steps=400,\n",
    "        warmup_steps=20,\n",
    "        output_path=(TMP_DIR / f\"./cross_encoder_{fold}\").as_posix(),\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on inference\n",
    "Next we need to use those pretrained cross-encoders to extract embeddings. Or, to use them to pretrain sentenceTransformer model (which produces object's embeddings)\n",
    "\n",
    "Ok, let's suppose we have molecular smiles and can precompute the embeddings.\n",
    "We can use them as a set of features for some complex model OR (and this is the simplest way) we can compute i.e., cos distance between train's active molecules and a target molecule, somehow aggregate (mean, max, median, ...) and decide if it is more active than inactive (we can also pick the same number of molecules from train set - maybe by training additional model to detect models similarity based on graph? and use them for computations).\n",
    "\n",
    "Let's start with the simplest case - comparing with all active and N = len(active) randomly picked from inactive. (For reproducibility during inference, we'll set seed to RANDOM_SEED before selecting them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d632e93423067b2b9b1b8d52846f85c868da433699adad534ad2cc6673775271"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('data_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
